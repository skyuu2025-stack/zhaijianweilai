
import React, { useState, useEffect, useRef } from 'react';
import { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';

const VoiceCompanionView: React.FC = () => {
  const [isActive, setIsActive] = useState(false);
  const [status, setStatus] = useState('ç‚¹å‡»å¼€å¯ AI è¯­éŸ³æ…°è—‰');
  const [summary, setSummary] = useState<string | null>(null);
  const [connectionStep, setConnectionStep] = useState<number>(0);
  
  const audioContextRef = useRef<AudioContext | null>(null);
  const inputContextRef = useRef<AudioContext | null>(null);
  const nextStartTimeRef = useRef(0);
  const sourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());
  const streamRef = useRef<MediaStream | null>(null);
  const fullConversationRef = useRef<{ role: string, text: string }[]>([]);

  const decode = (base64: string) => {
    const binaryString = atob(base64);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) bytes[i] = binaryString.charCodeAt(i);
    return bytes;
  };

  async function decodeAudioData(data: Uint8Array, ctx: AudioContext, sampleRate: number, numChannels: number): Promise<AudioBuffer> {
    const dataInt16 = new Int16Array(data.buffer);
    const frameCount = dataInt16.length / numChannels;
    const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);
    for (let channel = 0; channel < numChannels; channel++) {
      const channelData = buffer.getChannelData(channel);
      for (let i = 0; i < frameCount; i++) channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;
    }
    return buffer;
  }

  const startSession = async () => {
    setSummary(null);
    setConnectionStep(1);
    fullConversationRef.current = [];
    
    try {
      // 1. æŒ‰ç…§å®˜æ–¹ API è§„åˆ™ï¼šæ£€æŸ¥å¹¶æ‰“å¼€é€‰ Key å¼¹çª—
      // @ts-ignore
      if (window.aistudio && !(await window.aistudio.hasSelectedApiKey())) {
         // @ts-ignore
         await window.aistudio.openSelectKey();
      }

      setConnectionStep(2);
      setStatus('æ­£åœ¨è¯·æ±‚éº¦å…‹é£æƒé™...');
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;

      setConnectionStep(3);
      setStatus('åˆå§‹åŒ–éŸ³é¢‘å¼•æ“...');
      const inputCtx = new AudioContext({ sampleRate: 16000 });
      const outputCtx = new AudioContext({ sampleRate: 24000 });
      inputContextRef.current = inputCtx;
      audioContextRef.current = outputCtx;

      setStatus('å»ºç«‹åŠ å¯†ä¿¡é“...');
      setConnectionStep(4);
      
      const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
      // Fix: Updated model name to gemini-2.5-flash-native-audio-preview-12-2025
      const sessionPromise = ai.live.connect({
        model: 'gemini-2.5-flash-native-audio-preview-12-2025',
        callbacks: {
          onopen: () => {
            setIsActive(true);
            setConnectionStep(5);
            setStatus('æˆ‘åœ¨å¬ï¼Œè¿™é‡Œå¾ˆå®‰å…¨...');
            const source = inputCtx.createMediaStreamSource(stream);
            const scriptProcessor = inputCtx.createScriptProcessor(4096, 1, 1);
            scriptProcessor.onaudioprocess = (audioProcessingEvent) => {
              const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
              const l = inputData.length;
              const int16 = new Int16Array(l);
              for (let i = 0; i < l; i++) int16[i] = inputData[i] * 32768;
              const bytes = new Uint8Array(int16.buffer);
              let binary = '';
              for (let i = 0; i < bytes.byteLength; i++) binary += String.fromCharCode(bytes[i]);
              
              sessionPromise.then((session) => {
                session.sendRealtimeInput({ 
                  media: { data: btoa(binary), mimeType: 'audio/pcm;rate=16000' } 
                });
              });
            };
            source.connect(scriptProcessor);
            scriptProcessor.connect(inputCtx.destination);
          },
          onmessage: async (message: LiveServerMessage) => {
            const base64EncodedAudioString = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
            if (base64EncodedAudioString) {
              nextStartTimeRef.current = Math.max(nextStartTimeRef.current, outputCtx.currentTime);
              const audioBuffer = await decodeAudioData(decode(base64EncodedAudioString), outputCtx, 24000, 1);
              const source = outputCtx.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(outputCtx.destination);
              source.addEventListener('ended', () => sourcesRef.current.delete(source));
              source.start(nextStartTimeRef.current);
              nextStartTimeRef.current += audioBuffer.duration;
              sourcesRef.current.add(source);
            }

            if (message.serverContent?.inputTranscription?.text) {
              fullConversationRef.current.push({ role: 'user', text: message.serverContent.inputTranscription.text });
            }
            if (message.serverContent?.outputTranscription?.text) {
              fullConversationRef.current.push({ role: 'model', text: message.serverContent.outputTranscription.text });
            }

            if (message.serverContent?.interrupted) {
              sourcesRef.current.forEach(s => { try { s.stop(); } catch(e) {} });
              sourcesRef.current.clear();
              nextStartTimeRef.current = 0;
            }
          },
          onerror: (e: any) => {
            console.error('Live API Error:', e);
            setStatus('è¿æ¥æ³¢åŠ¨ï¼Œæ­£åœ¨é‡è¯•...');
            // @ts-ignore
            if (e.message?.includes("401") && window.aistudio) window.aistudio.openSelectKey();
            cleanup();
          },
          onclose: () => {
            setIsActive(false);
            setConnectionStep(0);
            setStatus('é€šè¯å·²ç»“æŸ');
          }
        },
        config: {
          responseModalities: [Modality.AUDIO],
          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } } },
          inputAudioTranscription: {},
          outputAudioTranscription: {},
          systemInstruction: 'ä½ æ˜¯ä¸€ä½æ¸©æš–ä¸”å¯Œæœ‰æ™ºæ…§çš„è§£å€ºå¿ƒç†é™ªæŠ¤ä¸“å®¶ã€‚è¯·ç”¨ç®€æ´ã€å¹³å’Œçš„è¯è¯­å®‰æŠšç”¨æˆ·çš„è´Ÿé¢æƒ…ç»ªã€‚'
        }
      });
    } catch (e: any) {
      console.error(e);
      setStatus('åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ');
      setConnectionStep(0);
    }
  };

  const endSession = async () => {
    setIsActive(false);
    setStatus('æ­£åœ¨ç”Ÿæˆç–—æ„ˆæŠ¥å‘Š...');
    cleanup();

    if (fullConversationRef.current.length > 0) {
      try {
        const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
        const historyText = fullConversationRef.current.map(c => `${c.role === 'user' ? 'ç”¨æˆ·' : 'ä¸“å®¶'}: ${c.text}`).join('\n');
        const response = await ai.models.generateContent({
          model: 'gemini-3-flash-preview',
          contents: [{ parts: [{ text: `æ ¹æ®å¯¹è¯æ€»ç»“ä¸€æ®µæå…¶æ¸©æš–ä¸”ç®€çŸ­çš„é¼“åŠ±ï¼ˆ80å­—å†…ï¼‰ï¼š\n\n${historyText}` }] }]
        });
        setSummary(response.text || 'æ¯ä¸€ä¸ªå‹‡æ•¢é¢å¯¹çš„ç¬é—´ï¼Œéƒ½æ˜¯é‡å›è‡ªç”±çš„å¼€å§‹ã€‚');
      } catch (err) {
        setSummary('æ„Ÿè°¢æ‚¨çš„å€¾è¯‰ã€‚è®°ä½ï¼Œæ‚¨å¹¶ä¸å­¤å•ã€‚');
      }
    } else {
      setSummary('é™é™çš„é™ªä¼´ä¹Ÿæ˜¯ä¸€ç§åŠ›é‡ã€‚å¦‚æœ‰éœ€è¦ï¼Œæˆ‘éšæ—¶éƒ½åœ¨ã€‚');
    }
    setConnectionStep(0);
  };

  const cleanup = () => {
    if (streamRef.current) streamRef.current.getTracks().forEach(track => track.stop());
    if (inputContextRef.current) inputContextRef.current.close().catch(() => {});
    if (audioContextRef.current) audioContextRef.current.close().catch(() => {});
    sourcesRef.current.forEach(s => { try { s.stop(); } catch(e) {} });
    sourcesRef.current.clear();
  };

  return (
    <div className="flex flex-col items-center justify-between h-[calc(100vh-140px)] py-4 px-6 text-center animate-fadeIn overflow-hidden">
      <div className="space-y-1 shrink-0">
        <h3 className="text-xl font-black text-white tracking-tight">{isActive ? 'æ­£åœ¨è†å¬æ‚¨çš„å€¾è¯‰' : summary ? 'ç–—æ„ˆåé¦ˆ' : '1V1 è¯­éŸ³ç–—æ„ˆ'}</h3>
        <p className={`text-[9px] font-bold uppercase tracking-[0.2em] transition-all ${isActive ? 'text-emerald-400' : 'text-slate-500'}`}>
          {status}
        </p>
        {connectionStep > 0 && !isActive && (
          <div className="w-16 h-0.5 bg-slate-800 rounded-full mx-auto mt-2 overflow-hidden">
             <div className="h-full bg-indigo-500 transition-all duration-500 animate-pulse" style={{ width: `${(connectionStep / 5) * 100}%` }}></div>
          </div>
        )}
      </div>

      <div className="flex-1 w-full flex flex-col items-center justify-center py-2">
        {summary ? (
          <div className="w-full animate-fadeIn bg-white/5 border border-white/10 p-6 rounded-[32px] text-left space-y-3 backdrop-blur-xl relative shadow-2xl">
            <div className="absolute -top-3 left-6 bg-indigo-600 px-3 py-1 rounded-full text-[8px] font-black uppercase tracking-widest text-white">AI ç–—æ„ˆå›å“</div>
            <p className="text-[13px] text-slate-200 leading-relaxed font-medium italic">{summary}</p>
            <button onClick={() => setSummary(null)} className="text-[9px] text-indigo-400 font-black uppercase underline">å…³é—­åé¦ˆ</button>
          </div>
        ) : (
          <div className={`w-32 h-32 rounded-full flex items-center justify-center transition-all duration-1000 ${isActive ? 'bg-indigo-600 animate-pulse shadow-[0_0_60px_rgba(79,70,229,0.3)]' : 'bg-slate-900 shadow-inner'}`}>
             {isActive ? (
                <div className="flex items-center gap-1">
                   {[1,2,3].map(i => <div key={i} className="w-1 h-6 bg-white rounded-full animate-bounce" style={{ animationDelay: `${i*0.2}s` }}></div>)}
                </div>
             ) : (
                <span className="text-5xl grayscale opacity-20">ğŸ›‹ï¸</span>
             )}
          </div>
        )}
      </div>

      <div className="w-full pb-28 shrink-0 space-y-4">
        {!isActive ? (
          <button 
            onClick={startSession} 
            disabled={connectionStep > 0}
            className="w-full bg-indigo-600 text-white py-5 rounded-[22px] font-black uppercase tracking-[0.25em] text-xs shadow-xl active:scale-95 transition-all disabled:opacity-50"
          >
            {connectionStep > 0 ? 'æ­£åœ¨åŒæ­¥...' : 'å¼€å¯é€šè¯'}
          </button>
        ) : (
          <button onClick={endSession} className="w-full bg-slate-900 border border-white/5 text-slate-400 py-5 rounded-[22px] font-black uppercase tracking-widest text-xs active:scale-95 transition-all">
            ç»“æŸé€šè¯
          </button>
        )}
        <p className="text-[7px] text-slate-600 font-bold uppercase tracking-widest">
          {isActive ? "ç«¯å¯¹ç«¯æµåŠ å¯†è¿è¡Œä¸­" : "é‡‡ç”¨ Native Audio çº§è¯­ä¹‰è¯†åˆ«"}
        </p>
      </div>
    </div>
  );
};

export default VoiceCompanionView;
